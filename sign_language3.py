# -*- coding: utf-8 -*-
"""sign_language3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uZR6eLW_xTI9ZiBap0v0IWCBO-FEs0S8
"""

import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import joblib
import warnings

# Load the Model and Preprocessors
print("Loading model and preprocessors...")

model = tf.keras.models.load_model('sign_language_model.h5')
scaler = joblib.load('scaler.joblib')
encoder = joblib.load('encoder.joblib')

#Initialize Mediapipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
# Set static_image_mode=False for video streams
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)

#Initialize OpenCV Webcam
cam = cv2.VideoCapture(0) # 0 is the default webcam

if not cam.isOpened():
    print("Error: Could not open webcam.")
    exit()

while cam.isOpened():
    ret, frame = cam.read()
    if not ret:
        print("Ignoring empty camera frame.")
        continue
    frame = cv2.flip(frame, 1)
    # Convert the BGR frame to RGB for Mediapipe
    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    # Process the frame to find hand landmarks
    results = hands.process(image_rgb)

    #Check if a Hand is Detected
    if results.multi_hand_landmarks:
        # Get the first detected hand
        hand_landmarks = results.multi_hand_landmarks[0]
        # Draw the hand skeleton on the frame
        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        try:
            landmark_list = []
            for landmark in hand_landmarks.landmark:
                landmark_list.append(landmark.x)
                landmark_list.append(landmark.y)
                landmark_list.append(landmark.z)

            # Reshape to (1, 63)
            landmark = np.array(landmark_list).reshape(1, -1)
            scaled = scaler.transform(landmark)

            #Make a Prediction
            prediction_probs = model.predict(scaled)
            predicted_index = np.argmax(prediction_probs)
        predicted_label=
        encoder.inverse_transform([predicted_index][0]
            confidence = np.max(prediction_probs)

            #Display the Result 
            cv2.rectangle(frame, (0, 0), (250, 60), (0, 0, 0), -1)
            cv2.putText(frame,
                        f'{predicted_label} ({confidence * 100:.0f}%)',
                        (10, 40), 
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1.2, (255, 255, 255), 2, cv2.LINE_AA)
        except Exception as e:
            print(f"Error during prediction: {e}")
            pass
    cv2.imshow('Sign Language Interpretation', frame)
    # Press 'q' to quit the loop
    if cv2.waitKey(5) & 0xFF == ord('q'):
        break
        
cam.release()
cv2.destroyAllWindows()
hands.close()
